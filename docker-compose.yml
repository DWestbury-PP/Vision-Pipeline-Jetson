# Vision Pipeline Jetson - Docker Compose Configuration
# 
# FULLY CONTAINERIZED ARCHITECTURE FOR NVIDIA JETSON:
# - All services run in Docker containers with CUDA GPU access
# - NVIDIA runtime provides GPU acceleration to containers
#
# Usage:
#   docker-compose up --build (starts all containerized services)
#
# Note: Requires NVIDIA Docker runtime to be properly configured

services:
  # Redis message bus
  redis:
    image: redis:7-alpine
    container_name: vision-pipeline-redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped
    networks:
      - vision-pipeline-network
    # PyTorch recommended flags
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864

  # Camera capture service
  # NOTE: Now runs in container with CSI/USB camera support
  camera:
    build:
      context: .
      dockerfile: containers/camera/Dockerfile.jetson
    container_name: vision-pipeline-camera
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CAMERA_TYPE=jetson_csi
      - CAMERA_INDEX=0
      - LOG_LEVEL=INFO
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - vision-pipeline-network
    # PyTorch recommended flags
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    # For camera access on Jetson
    privileged: true
    volumes:
      - /dev:/dev
      - /tmp/argus_socket:/tmp/argus_socket

  # YOLO object detection service
  # NOTE: Now runs in container with CUDA GPU acceleration
  yolo:
    build:
      context: .
      dockerfile: containers/yolo/Dockerfile.jetson
    container_name: vision-pipeline-yolo
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - YOLO_MODEL=yolo11n.pt
      - YOLO_DEVICE=cuda
      - LOG_LEVEL=INFO
    volumes:
      - ./models/yolo:/app/models/yolo
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - vision-pipeline-network
    # PyTorch recommended flags
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864

  # Moondream VLM service
  # NOTE: Now runs in container with CUDA GPU acceleration
  moondream:
    build:
      context: .
      dockerfile: containers/moondream/Dockerfile.jetson
    container_name: vision-pipeline-vlm
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - MOONDREAM_MODEL=vikhyatk/moondream2
      - MOONDREAM_DEVICE=cuda
      - VLM_FRAME_STRIDE=10
      - LOG_LEVEL=INFO
    volumes:
      - moondream_cache:/home/appuser/.cache/huggingface
      - ./models/moondream:/app/models/moondream
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - vision-pipeline-network
    # PyTorch recommended flags
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864

  # Fusion service
  fusion:
    build:
      context: .
      dockerfile: containers/fusion/Dockerfile.jetson
    container_name: vision-pipeline-fusion
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LOG_LEVEL=INFO
    depends_on:
      redis:
        condition: service_healthy
      camera:
        condition: service_started
      yolo:
        condition: service_started
      moondream:
        condition: service_started
    restart: unless-stopped
    networks:
      - vision-pipeline-network
    # PyTorch recommended flags
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864

  # API service
  api:
    build:
      context: .
      dockerfile: containers/api/Dockerfile.jetson
    container_name: vision-pipeline-api
    ports:
      - "8000:8000"
      - "8001:8001"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - WEBSOCKET_PORT=8001
      - LOG_LEVEL=INFO
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - vision-pipeline-network
    # PyTorch recommended flags
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864

  # Frontend service
  frontend:
    build:
      context: .
      dockerfile: containers/frontend/Dockerfile
    container_name: vision-pipeline-frontend
    ports:
      - "3000:80"
    environment:
      - API_BASE_URL=http://localhost:8000
    depends_on:
      - api
    restart: unless-stopped
    networks:
      - vision-pipeline-network
    # PyTorch recommended flags
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864

volumes:
  redis_data:
  moondream_cache:

networks:
  vision-pipeline-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16
